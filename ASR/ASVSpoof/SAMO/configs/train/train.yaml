# Training hyperparameters
defaults:
  - _self_

num_epochs: 100
save_interval: 5 # save checkpoint for every how many epoch
update_interval: 3
interval: 1
gpu_index: 0

optim:
  _target_: torch.optim.Adam
  lr: 0.0001
  betas: [0.9, 0.999]
  weight_decay: 0.0001
  eps: 1e-8
  amsgrad: false

scheduler: 
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${..num_epochs}
  eta_min: 0.000005
